# Latent Action Model (LAM) Configuration
# For Pong dataset with 2 actions (up/down)
# This forces the model to learn more compressed representations

model:
  name: "lam"
  
  encoder:
    num_layers: 8
    d_model: 512
    num_heads: 8
    k_q_size: 64
    dim_feedforward: 2048  # 4 * d_model
    dropout: 0.1
    activation: "gelu"
  
  decoder:
    num_layers: 8
    d_model: 512
    num_heads: 8
    k_q_size: 64
    dim_feedforward: 2048  # 4 * d_model
    dropout: 0.1
    activation: "gelu"
  
  codebook:
    num_codes: 2  # Only 2 actions for pong (up/down)
    latent_dim: 32
    commitment_cost: 0.25
    decay: 0.99
    epsilon: 1e-5
  
  patch_size: 16

data:
  sequence_length: 16
  fps: 10
  resolution: [128, 72, 3]  # [H, W, C]
  batch_size: 4
  num_workers: 4

training:
  max_lr: 3.0e-4
  min_lr: 3.0e-4
  beta1: 0.9
  beta2: 0.9
  weight_decay: 1.0e-4
  warmup_steps: 2000
  max_steps: 20000
  lr_schedule: "cosine"
  
  # Loss weights
  reconstruction_weight: 1.0
  commitment_weight: 0.25
  codebook_weight: 1.0
  
  # Training settings
  mixed_precision: true
  gradient_checkpointing: true
  max_grad_norm: 1.0
  save_every: 5000
  eval_every: 1000

output:
  checkpoint_dir: "checkpoints/lam"
  log_dir: "logs/lam"
