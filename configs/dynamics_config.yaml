# Dynamics Model Configuration
# Scaled down for 16GB GPU

model:
  name: "dynamics"
  
  architecture:
    num_layers: 8  # Balanced: reduced from 12 for memory, but more than 6 for better capacity
    d_model: 640  # Balanced: between 512 (too small) and 768 (original)
    num_heads: 8  # Balanced: 8 heads
    k_q_size: 256  # Must be divisible by num_heads (8 * 32 = 256, head_dim = 32)
    dim_feedforward: 2560  # 4 * d_model
    dropout: 0.1
    activation: "gelu"
    qk_normalization: true
  
  token_embedding:
    vocab_size: 512  # Must match tokenizer codebook num_codes (was 1024, but tokenizer uses 512)
    embedding_dim: 640  # Must match d_model
  
  action_embedding:
    num_actions: 8
    embedding_dim: 640  # Must match d_model

data:
  sequence_length: 12  # Balanced: reduced from 16 for memory, but more than 8 for better temporal modeling
  batch_size: 1  # Reduced for memory (use --batch_size to override)
  num_workers: 0  # Reduced for memory efficiency

training:
  max_lr: 3.0e-5
  min_lr: 3.0e-6
  beta1: 0.9
  beta2: 0.9
  weight_decay: 1.0e-4
  warmup_steps: 1000  # Scaled from 5k
  max_steps: 10000  # Scaled from 125k
  lr_schedule: "cosine"
  
  # MaskGIT training
  mask_prob: 0.5  # Bernoulli rate 0.5-1.0
  min_mask_prob: 0.5
  max_mask_prob: 1.0
  
  # Training settings
  mixed_precision: true
  gradient_checkpointing: true
  max_grad_norm: 1.0
  save_every: 500  # Save more frequently (was 2000)
  eval_every: 250
  
  # Memory management (dynamics uses 3 models, needs aggressive cleanup)
  memory_offload_interval: 50
  aggressive_cleanup_interval: 10
  memory_threshold_gb: 7.0

inference:
  maskgit_steps: 25
  temperature: 2.0
  sampling: "random"

output:
  checkpoint_dir: "checkpoints/dynamics"
  log_dir: "logs/dynamics"
