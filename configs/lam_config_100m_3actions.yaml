# Latent Action Model (LAM) Configuration
# Optimized for ~100 Million parameters with 3 actions (up, down, do nothing)
# For Pong dataset - matches actual game actions and prevents codebook collapse

model:
  name: "lam"
  
  encoder:
    num_layers: 10  # Increased from 6 (~67% more layers)
    d_model: 768  # Increased from 384 (2x, ~4x memory in attention)
    num_heads: 12  # Increased from 6 (proportional to d_model)
    k_q_size: 96  # Increased from 48 (proportional to d_model)
    dim_feedforward: 3072  # Increased from 1536 (4 * d_model, 2x)
    dropout: 0.1
    activation: "gelu"
  
  decoder:
    num_layers: 10  # Increased from 6 (~67% more layers)
    d_model: 768  # Increased from 384 (2x, matches encoder)
    num_heads: 12  # Increased from 6 (proportional to d_model)
    k_q_size: 96  # Increased from 48 (proportional to d_model)
    dim_feedforward: 3072  # Increased from 1536 (4 * d_model, 2x)
    dropout: 0.1
    activation: "gelu"
  
  codebook:
    num_codes: 3  # 3 actions for pong: up, down, do nothing (instead of 8)
    latent_dim: 32  # Keep same
    commitment_cost: 0.25
    decay: 0.99
    epsilon: 1e-5
  
  patch_size: 16  # Keep same

data:
  sequence_length: 12  # Keep same as 100m config
  fps: 10
  resolution: [128, 72, 3]  # [H, W, C] - keep same resolution
  batch_size: 1  # Reduced from 2 (larger model needs more memory per sample)
  num_workers: 2  # Keep same

training:
  max_lr: 3.0e-4
  min_lr: 3.0e-4
  beta1: 0.9
  beta2: 0.9
  weight_decay: 1.0e-4
  warmup_steps: 2000
  max_steps: 20000
  lr_schedule: "cosine"
  
  # Loss weights
  reconstruction_weight: 1.0
  commitment_weight: 0.25
  codebook_weight: 1.0
  
  # Training settings
  mixed_precision: true  # CRITICAL: Keep enabled for memory savings
  gradient_checkpointing: true  # CRITICAL: Keep enabled for memory savings
  max_grad_norm: 1.0
  save_every: 5000
  eval_every: 1000
  
  # Memory management (tuned for larger model)
  memory_offload_interval: 100  # More frequent offloading
  aggressive_cleanup_interval: 10  # More frequent cleanup
  memory_threshold_gb: 9.0  # Trigger cleanup at 9GB

output:
  checkpoint_dir: "checkpoints/lam"
  log_dir: "logs/lam"
